custom:

  # Cluster configs for each environment
  default-cluster-spec: &default-cluster-spec
    spark_version: '11.0.x-cpu-ml-scala2.12'
    node_type_id: 'i3.xlarge' # NOTE: this is an AWS-specific instance type. Change accordingly if running on Azure or GCP.
    driver_node_type_id: 'i3.xlarge'  # NOTE: this is an AWS-specific instance type. Change accordingly if running on Azure or GCP.
    num_workers: 1

  cpu-cluster-config: &cpu-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  basic-cluster-props: &basic-cluster-props
    spark_version: "11.0.x-gpu-ml-scala2.12"

  gpu-static-cluster: &gpu-static-cluster
    new_cluster:
      <<: *basic-cluster-props
      num_workers: 0
      node_type_id: "g4dn.xlarge"
      driver_node_type_id: "g4dn.xlarge"
      spark_conf: 
        spark.master: "local[*, 4]"
      aws_attributes:
        first_on_demand: 1
        availability: SPOT_WITH_FALLBACK
        zone_id: us-west-2a
        instance_profile_arn: arn:aws:iam::997819012307:instance-profile/oetrta-IAM-access
      enable_elastic_disk: true
      single_user_name: puneet.jain@databricks.com
      data_security_mode: SINGLE_USER
      runtime_engine: STANDARD


# please note that we're using FUSE reference for config file, hence we're going to load this file using its local FS path
environments:
  default:
    strict_path_adjustment_policy: true
    jobs:
      - name: "nlp_sa_training_step"
        <<:
          - *gpu-static-cluster
        notebook_task:
          notebook_path: "/Repos/puneet.jain@databricks.com/transformer-nlp-solution-accelarator_dbx/main_db"
          source: "WORKSPACE"

      - name: 'hugging-face-model-train'
        <<:
          - *cpu-cluster-config
        spark_python_task:
          python_file: 'file://nlp_sa/pipelines/model_train_job.py'
          parameters: [ '--base-env', 'file:fuse://conf/.base.env',
                        '--conf-file', 'file:fuse://conf/pipeline_configs/model_train_multi_class.yaml' ]

      - name: 'hugging-face-model-train-shared-cluster' #use this profile to execute job in interactive cluster
        spark_python_task:
          python_file: 'file://nlp_sa/pipelines/model_train_job.py'
          parameters: [ '--base-env', 'file:fuse://conf/.base.env',
                        '--conf-file', 'file:fuse://conf/pipeline_configs/model_train_multi_class.yaml' ]
